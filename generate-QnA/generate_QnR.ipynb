{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e05e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# default parameter\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "INPUT_FILE = \".crawling-siteweb/out/result.json\"\n",
    "OUTPUT_FILE = \"./out/qa_dataset.jsonl\"\n",
    "MODEL_NAME = \"llama3-8b-8192\"\n",
    "NUM_QNR = 10 # parameter for number of pairs QnR\n",
    "# Too many Q&A pairs may exceed token limit → keep it small (e.g. ≤ 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b83eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " URL: https://python.langchain.com/docs/introduction/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read content from result.json\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "content = data[0][\"content\"]\n",
    "url = data[0][\"url\"]\n",
    "print(f\"\\n URL: {url}\\n\")\n",
    "\n",
    "# send QnR request to Groq\n",
    "prompt = f\"\"\"\n",
    "Here is technical context:\n",
    "\n",
    "{content}\n",
    "\n",
    "Generate {NUM_QNR} pairs of Question-Response that are clear and helpful based on this content.\n",
    "Format:\n",
    "Question: ...\n",
    "Response: ...\n",
    "\"\"\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"temperature\": 0.7, #Sufficiently creative 0.5<t<1.0\n",
    "    \"max_tokens\": 1024\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4b22a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Q: What is LangChain?\n",
      " A: LangChain is a framework for developing applications powered by large language models (LLMs).\n",
      "\n",
      "\n",
      " Q: How do I build a simple LLM application?\n",
      " A: Check out the \"Build a Simple LLM Application\" tutorial in the LangChain tutorials section.\n",
      "\n",
      "\n",
      " Q: What are the key components of LangChain?\n",
      " A: LangChain consists of multiple open-source libraries, including langchain-core, langchain, langchain-community, and langgraph.\n",
      "\n",
      "\n",
      " Q: How do I integrate LangChain with a specific provider?\n",
      " A: Check out the integrations page for a list of providers and their corresponding integration packages.\n",
      "\n",
      "\n",
      " Q: What is LangGraph, and how is it related to LangChain?\n",
      " A: LangGraph is an orchestration framework that combines LangChain components into production-ready applications with persistence, streaming, and other key features.\n",
      "\n",
      "\n",
      " Q: How do I evaluate and optimize my LangChain application?\n",
      " A: Use LangSmith to inspect, monitor, and evaluate your applications, and continuously optimize and deploy with confidence.\n",
      "\n",
      "\n",
      " Q: What are the key concepts I need to know for LangChain?\n",
      " A: Check out the Conceptual Guide for high-level explanations of all LangChain concepts.\n",
      "\n",
      "\n",
      " Q: How do I get started with LangChain?\n",
      " A: Start with the Tutorials section, which includes guides for building a simple LLM application, a chatbot, and an agent.\n",
      "\n",
      "\n",
      " Q: What are the different ways to deploy my LangChain application?\n",
      " A: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\n",
      "\n",
      "\n",
      " Q: How can I contribute to the LangChain ecosystem?\n",
      " A: Check out the Contributing guide for guidelines on contributing and help getting your dev environment set up.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=payload)\n",
    "result = response.json()\n",
    "answer_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# parsing response from groq\n",
    "lines = answer_text.strip().split('\\n')\n",
    "all_qas = []\n",
    "question, answer = None, None\n",
    "\n",
    "for line in lines:\n",
    "    if line.lower().startswith(\"question:\"):\n",
    "        question = line.split(\":\", 1)[1].strip()\n",
    "    elif line.lower().startswith(\"response:\"):\n",
    "        answer = line.split(\":\", 1)[1].strip()\n",
    "        if question and answer:\n",
    "            all_qas.append({\n",
    "                \"instruction\": question,\n",
    "                \"input\": \"\",\n",
    "                \"output\": answer\n",
    "            })\n",
    "            print(f\"\\n Q: {question}\\n A: {answer}\\n\")\n",
    "            question, answer = None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5780f7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 Q-R pairs saved to ./out/qa_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save result\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in all_qas:\n",
    "        f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\n{len(all_qas)} Q-R pairs saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdce23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
